runs:
    # Test as a standard reference
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --enable-torch-compile
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--enable-torch-compile --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend fa3
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--attention-backend fa3 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend flashinfer
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--attention-backend flashinfer --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend triton
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--attention-backend triton --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend torch_native
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--attention-backend torch_native --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size -1
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size -1 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size 2048 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 4096
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size 4096 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 8192
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size 8192 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 2048
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size 2048 --max-prefill-tokens 2048 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 4096
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size 2048 --max-prefill-tokens 4096 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 8192
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size 2048 --max-prefill-tokens 8192 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    
    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 2048
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--chunked-prefill-size 2048 --enable-mixed-chunk --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2


# **************

  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --enable-torch-compile
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --enable-torch-compile --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend fa3
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --attention-backend fa3 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend flashinfer
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --attention-backend flashinfer --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend triton
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --attention-backend triton --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --attention-backend torch_native
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --attention-backend torch_native --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size -1
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size -1 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size 2048 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 4096
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size 4096 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 8192
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size 8192 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 2048
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size 2048 --max-prefill-tokens 2048 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 4096
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size 2048 --max-prefill-tokens 4096 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 8192
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size 2048 --max-prefill-tokens 8192 --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2

    
    # Test with --chunked-prefill-size 2048 --max-prefill-tokens 2048
  - engine:
      - type: sglang
        model: NousResearch/Meta-Llama-3.1-8B
        args: "--quantization fp8 --chunked-prefill-size 2048 --enable-mixed-chunk --tp 4"
        env: {}
    benchmarks:
      - type: conversational_short
        dataset-name: random
        random-input-len: 100
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 50
        request-rate: 10
      - type: conversational_medium
        dataset-name: random
        random-input-len: 1000
        random-output-len: 100
        random-prefix-len: 2000
        num-prompts: 50
        request-rate: 5
      - type: conversational_long
        dataset-name: random
        random-input-len: 5000
        random-output-len: 100
        random-prefix-len: 7000
        num-prompts: 50
        request-rate: 2
      - type: summarization
        dataset-name: random
        random-input-len: 12000
        random-output-len: 100
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: write_essay
        dataset-name: random
        random-input-len: 100
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2
      - type: rewrite_essay
        dataset-name: random
        random-input-len: 12000
        random-output-len: 3000
        random-prefix-len: 0
        num-prompts: 20
        request-rate: 2